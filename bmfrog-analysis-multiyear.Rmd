---
title: "BMFrog Analysis"
output: html_document
date: "2024-03-07"
---

# Brief

This document is intended as supplementary material to the analysis of acoustic frog data between 2018 and 2024 (6 survey years). This document provides relevant R code to process data from the acoustic classifier, extract relevent environmental variables, run a multi-season multi-species occupancy model and interrogate the results.

# Setup

Load relevant R packages and state settings for modeling. 

```{r setup}
library(tidyverse)
library(sf)
library(mapview)
library(terra)
library(readr)
library(readxl)
library(cmdstanr)
library(bayesplot)
library(httr)
library(VicmapR)
library(ggridges)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
options(mc.cores=4)
register_knitr_engine(override = FALSE)
```

Source a range of handy functions for summarily and plotting outputs from a STAN model. These were used in a different project and are stored on github. 

```{r sourcefunctions, results = 'hide', echo =FALSE, message=FALSE}
# Source useful functions from the deer project github page
req <- GET("https://api.github.com/repos/JustinCally/statewide-deer-analysis/git/trees/main?recursive=1")
stop_for_status(req)
filelist <- unlist(lapply(content(req)$tree, "[", "path"), use.names = F)
function_files <- paste0("https://raw.githubusercontent.com/JustinCally/statewide-deer-analysis/main/",
                         grep("functions/", filelist, value = TRUE, fixed = TRUE))
sapply(function_files, source, verbose = F)
```

# Site data  

## Site metadata

Below we load in site metadata for the analysis, which includes the location and deployment dates for the audiomoths. 

```{r SiteData}
site_metadata <- read_excel("data/TLM_Barmah-Millewa_AM_deployment_metadata_all_years.xlsx")

# site_data_habitat <- read_excel("data/site_data/TLM_habitat_transect_data_all_years.xlsx") %>%
#   group_by(Site, Date) %>%
#   summarise(across(c("Depth", "Emergent macrophyte", "Submerged macrophyte",
#                      "Floating macrophyte", "CWD", "Bare ground"), mean)) %>%
#   mutate(Year = as.integer(as.factor(year(Date))))
# 
# site_data_habitat2 <- read_excel("data/site_data/TLM_water_quality_all_years.xlsx") %>%
#   group_by(Site, Date) %>%
#   summarise(across(c("Waterbody width (m)", "Waterbody length (m)", "Max depth (cm)",
#                      "Canopy cover (%)", "Conductivity", "pH", "Water temp (°C)",
#                      "Oxygen (mg/L)", "Oxygen sat (%)", "mmHg", "Turbidity (FNU)",
#                      "Turbidity (NTU)"), mean)) %>%
#   mutate(Year = as.integer(as.factor(year(Date))))

# Read in water management area data 
WMAs <- read_excel("data/WMAs/Sites_WMAs.xlsx")
WMA_scores <- read_excel("data/WMAs/WMA_flood_scores.xlsx")

WMA_joined <- left_join(WMAs, WMA_scores) %>% 
  select(WMA, `2018-19` = `2018`, `2019-20` = `2019`, `2020-21` = `2020`, `2021-22` = `2021`, `2022-23` = `2022`, `2023-24` = `2023`) %>% 
  pivot_longer(2:7, names_to = "Monitoring year", values_to = "FloodScore") %>%
  distinct()

# Convert site metadata to a spatially explicit dataset and format
site_metadata_sf <- site_metadata %>%
  left_join(WMA_joined, by = join_by(WMA, `Monitoring year`)) %>% 
  st_as_sf(coords = c("X", "Y"), crs = 28355) %>%
  mutate(`Site name` = case_when(`Site` == "Warri" ~ "Warri Yards", 
                                 `Site` == "Hughes Crossing" ~ "Hughs Crossing",
                                 Site == "Little Rushy Swamp" ~ "Little Rushy",
                                 Site == "Reed Beds Swamp" ~ "Reed Bed Swamp",
                                 Site == "Wathours Lagoon" ~ "Wathours",
                                 TRUE ~ `Site`), 
         Site = paste(`Site name`, AM, sep = "_AM"), 
         Site_Year = paste(Site, `Monitoring year`, sep = "_"),
         Year = factor(`Monitoring year`) %>% as.integer(),
         Cluster = factor(`Site name`) %>% as.integer(),
         Loc = factor(Site) %>% as.integer(),
         No_survey_nights = as.numeric(No_PAM_nts)) %>%
  # arrange(`Site`) %>%
  filter(!is.na(No_survey_nights) & No_survey_nights > 0) %>%
  arrange(Site_Year) %>%
  filter(!(Site_Year %in% c("Rookery_AM1_2022-23", "Two Mile Bunyip_AM2_2021-22"))) 

unique_sites <- site_metadata_sf %>% 
  group_by(Site) %>%
  slice(1) %>%
  select(Site)

# get alist of the nights deployed 
nights_dep <- list()
for(i in 1:nrow(site_metadata_sf)) {
  nights_dep[[i]] <- data.frame(Site = site_metadata_sf$Site[i], 
                                Site_Year = site_metadata_sf$Site_Year[i], 
                           Date = seq.Date(from = as.Date(site_metadata_sf$`Date start`[i]), 
                                           by = "1 day", 
                                           length.out = site_metadata_sf$No_survey_nights[i])) #site_metadata_sf$No_survey_nights[i]))
}
nights_dep_joined <- bind_rows(nights_dep)
```

## Generate a hull for the study area

```{r hull, eval = F}
site_hull <- st_convex_hull(unique_sites %>% st_combine()) %>% 
  st_transform(3577) %>%
  st_buffer(5000)
hydro_data <- vicmap_query("open-data-platform:hy_water_area_polygon") %>%
  filter(BBOX(site_hull)) %>%
  collect()

mapview(hydro_data, zcol = "feature_type_code") + mapview(site_metadata_sf)

sf::st_write(site_hull, "data/barmah_hull/barmah_hull.shp")
sf::st_write(unique_sites %>% st_transform(3577) %>% st_buffer(100), "data/site_metadata_sf/site_metadata_sf.shp", overwrite = T)
```

## Hydrological data 

Read in the hydrological data. This data has been obtained through Digital Earth Austraia sandbox (https://app.sandbox.dea.ga.gov.au/), the code used to extract the hydrologial data and the csvs for each site are available on GitHub: https://github.com/JustinCally/dea-barmah  

```{r hydro, message = F, warning = F, eval = F}
# load in csv files of hydro conditions 
# list.files("https://github.com/JustinCally/dea-barmah/tree/main/output")
# req <- GET("https://api.github.com/repos/JustinCally/dea-barmah/git/trees/main?recursive=1")
# stop_for_status(req)
# filelist <- unlist(lapply(content(req)$tree, "[", "path"), use.names = F)
# csv_files <- paste0("https://raw.githubusercontent.com/JustinCally/dea-barmah/main/",
#                          grep("output/", filelist, value = TRUE, fixed = TRUE))
# csv_files <- grep(".ipynb_checkpoints", x = csv)
# 
# sapply(function_files, source, verbose = F)

#paste
csv_files <- paste0("https://raw.githubusercontent.com/JustinCally/dea-barmah/main/output/", unique(site_metadata_sf$Site), ".csv")
csv_files <- stringr::str_replace_all(csv_files, pattern = " ", replacement = "%20")
hydryo_data <- list()
for(i in 1:length(csv_files)) {
  hydryo_data[[i]] <- readr::read_csv(csv_files[i], show_col_types = F) %>%
    mutate(Site = unique(site_metadata_sf$Site)[i])
}

hydro_data_combined <- bind_rows(hydryo_data)
```

Given that the data was often only captured every 2 weeks we want to obtain daily estimates of hydrological conditions. To this we interpolate the hydrological data across the time range of the study (2018 - early 2024). We do this by fitting a polynomial regression with the `loess()` function. 

```{r interpolation, eval = F}
#hydro interpolation 

new_df_raw <- data.frame(date = seq(from = min(hydro_data_combined$date),
                                to = max(hydro_data_combined$date),
                                by="day"))

interpolate_data <- list()

for(i in 1:length(csv_files)) {
  
  hd <- hydryo_data[[i]]
  
  mod_pv <- loess(pv ~ as.numeric(date), data = hd, span = 0.075)
  mod_npv <- loess(npv ~ as.numeric(date), data = hd, span = 0.075)
  mod_bs <- loess(bs ~ as.numeric(date), data = hd, span = 0.075)
  mod_wet <- loess(wet ~ as.numeric(date), data = hd, span = 0.075)
  mod_water <- loess(water ~ as.numeric(date), data = hd, span = 0.075)
  mod_veg_areas <- loess(veg_areas ~ as.numeric(date), data = hd, span = 0.075)


interpolate_data[[i]] <- new_df_raw %>%
  mutate(Site = unique(site_metadata_sf$Site)[i],
         pv = predict(mod_pv, newdata = new_df_raw),
         npv = predict(mod_npv, newdata = new_df_raw),
         bs = predict(mod_bs, newdata = new_df_raw),
         wet = predict(mod_wet, newdata = new_df_raw),
         water = predict(mod_water, newdata = new_df_raw),
         veg_areas = predict(mod_veg_areas, newdata = new_df_raw)) %>%
  mutate(across(.cols = c(pv,npv,bs, wet, water, veg_areas), 
                .fns = ~ case_when(.x > 1 ~ 1, 
                                   .x < 0 ~ 0,
                                   TRUE ~ .x)))

}

interpolate_combined <- bind_rows(interpolate_data) %>%
  mutate(Date = as.Date(date)) %>%
  select(-date)

interpolate_data[[62]] %>%
  ggplot() +
  geom_point(aes(x = date, y = pv)) +
  ylim(c(0,1))
```

## Daily climate variables  

Using the National Oceanic and Atmospheric Administration (NOAA) data of gridded daily precipitation and temperature, we extract the daily precipitation and minimum temperature values estimated at each site at a broad spatial scale (0.5 x 0.5 degrees).

```{r precip, eval = F}
# load in daily precip 
# https://psl.noaa.gov/data/gridded/data.cpc.globalprecip.html
precipitation_daily <- c(terra::rast("data/climate/precip_2018.nc"), 
                         terra::rast("data/climate/precip_2019.nc"), 
                         terra::rast("data/climate/precip_2020.nc"),
                         terra::rast("data/climate/precip_2021.nc"), 
                         terra::rast("data/climate/precip_2022.nc"), 
                         terra::rast("data/climate/precip_2023.nc"),
                         terra::rast("data/climate/precip_2024.nc"))
precipitation_daily_df <- bind_cols(site_metadata_sf$Site_Year, 
                                    extract(precipitation_daily, vect(site_metadata_sf)))
colnames(precipitation_daily_df) <- c("Site_Year","ID", as.character(seq.Date(from = as.Date("2018-01-01"), 
                                                     to = as.Date("2024-05-13"), by = "day")))

precipitation_daily_long <- pivot_longer(precipitation_daily_df, 
                                         cols = 3:2327, names_to = "Date", values_to = "Precipitation") %>%
  select(-ID) %>%
  mutate(Date = as.Date(Date)) %>%
  right_join(nights_dep_joined, by = join_by(Site_Year, Date))
```

```{r temp, eval = F}
# load in daily precip 
# https://psl.noaa.gov/data/gridded/data.cpc.globalprecip.html
tmin_daily <- c(terra::rast("data/climate/precip_2018.nc"), 
                         terra::rast("data/climate/tmin_2019.nc"), 
                         terra::rast("data/climate/tmin_2020.nc"),
                         terra::rast("data/climate/tmin_2021.nc"), 
                         terra::rast("data/climate/tmin_2022.nc"), 
                         terra::rast("data/climate/tmin_2023.nc"),
                         terra::rast("data/climate/tmin_2024.nc"))
tmin_daily_df <- bind_cols(site_metadata_sf$Site_Year, 
                                    extract(tmin_daily, vect(site_metadata_sf)))
colnames(tmin_daily_df) <- c("Site_Year","ID", as.character(seq.Date(from = as.Date("2018-01-01"), 
                                                     to = as.Date("2024-05-13"), by = "day")))

tmin_daily_long <- pivot_longer(tmin_daily_df, 
                                cols = 3:2327, 
                                names_to = "Date", 
                                values_to = "tmin") %>%
  select(-ID) %>%
  mutate(Date = as.Date(Date)) %>%
  right_join(nights_dep_joined, by = join_by(Site_Year, Date))
```

## Save daily variables 

Given some of the preceding code can take a while to run we save the output as an rds. This output is the daily values at each site for hydrological and climate conditions as well as the days since the start of spring (1st of September). 

```{r dailyvars, eval = F}
daily_vars <- left_join(precipitation_daily_long, 
                        tmin_daily_long) %>%
  mutate(Day = lubridate::yday(Date)-243, 
         Day = case_when(Day < 1 ~ Day + 243 + (365-243), 
                         TRUE ~ Day),
         cosDay = 2*pi*Day/365, 
         sinDay = 2*pi*Day/365) %>% 
  inner_join(interpolate_combined, by = c("Site", "Date")) %>%
  distinct()

saveRDS(daily_vars, "data/intermediate/daily_vars.rds")
```

## Load daily variables

```{r loaddailyvars}
# read in daily vars
daily_vars <- readRDS("data/intermediate/daily_vars.rds")

site_vars <- daily_vars %>%
  group_by(Site_Year) %>%
  summarise(across(c("pv", "npv", "bs", "wet", "water", "veg_areas"), mean)) %>% 
  ungroup() %>%
  mutate(watercomb = wet + water) %>%
  left_join(site_metadata_sf %>% st_drop_geometry() %>% transmute(Site_Year, WMA, FloodScore = factor(FloodScore)))
```

# Acoustic data 

## Daily acoustic data

For this analysis we model occupancy and call rate using daily maximum call probability scores. We obtained these scores from the raw model output and processed them in a separate script available in this repository: `frog_daily_subset.R`. This data comes in two chunks (2018-2021 and 2021-2024). 

```{r dailymaxscores}
# Subset to site years
site_year <- nights_dep_joined %>% select(Site, Site_Year, Date) %>% 
  unique() %>% 
  left_join(site_metadata_sf %>% st_drop_geometry() %>% select(Site_Year, Year, Cluster, Loc), by = join_by(Site_Year))

# List of species we have data for

species_list <- c(`Barking Marsh Frog` = 13059,
                  `Eastern Sign-bearing Froglet` = 13131,
                  `Common Froglet` = 13134,
                  # `Sloane's Froglet` = 13135,
                  `Common Spadefoot Toad` = 13086,
                  `Peron's Tree Frog` = 13204,
                  `Pobblebonk Frog` = 63913,
                  `Spotted Marsh Frog (race unknown)` = 13063)

nspecies <- length(species_list)

sp_l_col <- paste0("sp_", species_list)

# species_cols <- str_extract_all(last(colnames(site_records)), "[-+]?[0-9]+")[[1]] 

site_records_all <- list()
daily_files <- list.files("data/daily_data/OtherYears/", full.names = T)
files_to_read <- list.files("data/daily_data/OtherYears/")
sp_read_order <- stringr::str_remove_all(files_to_read, c("daily_max_|.csv"))

for(i in 1:length(daily_files)) {
  site_records_all[[i]] <- read_csv(daily_files[i], show_col_types = F) %>% 
    select(-1) %>% 
    mutate(DateTime = as.POSIXct(DateTime, tz = Sys.timezone()), 
           Date = coalesce(Date, as.Date(DateTime - hours(12)))) %>%
    mutate(`Site` = case_when(Site == "Little Rushy Swamp_AM1" ~ "Little Rushy_AM1",
                              Site == "Little Rushy Swamp_AM2" ~ "Little Rushy_AM2",
                              TRUE ~ `Site`)) %>%
    mutate(Validated_Grp = as.character(species_list[[sp_read_order[i]]]),
           Validation_Species = sp_read_order[i],
           Validated_Grp_score = !!sym(sp_read_order[i])) %>%
    select(FileId, Filename, Site, Date, DateTime, Orig_Start, Orig_End, 
           Validated_Grp, Validation_Species, Validated_Grp_score) %>% 
    left_join(site_year) %>%
    filter(!is.na(Year))
}

names(site_records_all) <- sp_read_order
 
#### 2021-24 ####
site_records_2023 <- list()
daily_files <- list.files("data/daily_data/2021-2024/", full.names = T)
files_to_read <- list.files("data/daily_data/2021-2024/")
sp_read_order <- stringr::str_remove_all(files_to_read, c("daily_max_|.csv"))

for(i in 1:length(daily_files)) {
  site_records_2023[[i]] <- read_csv(daily_files[i], show_col_types = F) %>% 
    select(-1) %>% 
    mutate(Validated_Grp = as.character(species_list[[sp_read_order[i]]]),
           Validation_Species = sp_read_order[i],
           Validated_Grp_score = !!sym(sp_read_order[i])) %>% 
    select(FileId, Filename, Site, Date, DateTime, Orig_Start, Orig_End, 
           Validated_Grp, Validation_Species, Validated_Grp_score) %>% 
    left_join(site_year) %>%
    filter(!is.na(Year))
}

names(site_records_2023) <- sp_read_order

site_records_combined <- list()

for(x in names(species_list)) {
  site_records_combined[[x]] <- bind_rows(site_records_all[[x]], site_records_2023[[x]]) %>%
    mutate(SnippetID = paste(FileId, Site, Date, Orig_Start, Orig_End, sep = "_"))
}



```

# Validated site records  

Up to 500 daily calls for each species were validated, with validations for each species covering ranges of probability scores from 0 to 1. We read in these validations and visualise the distribution for 'true positives' and 'false positives'.

```{r validatedscores, fig.width = 8, fig.height = 5, fig.cap = "Distributions of probability scores assigned by the classifier"}
# site_validation <- read_excel("data/TLM_Barmah-Millewa_2022-23_validations_ALL.xlsx") %>%
#  mutate(Site = case_when(Site == "Warri" ~ "Warri Yards", 
#                          TRUE ~ Site),
#         Site = paste(Site, AM, sep = "_AM"))

files_to_read <- list.files("data/validated_calls/OtherYears")
sp_read_order <- stringr::str_remove(files_to_read, "_validation.csv")

validated_calls <- list()
for(i in 1:nspecies) {
  validated_calls[[sp_read_order[i]]] <- readr::read_csv(paste0("data/validated_calls/OtherYears/", files_to_read[i]), 
                                                         show_col_types = F) %>%
    mutate(Validated_Grp = as.character(species_list[[sp_read_order[i]]]),
           Validation_Species = sp_read_order[i],
           Validated_Grp_score = !!sym(sp_read_order[i]), 
           Date = as.Date(Date, "%d/%m/%Y")) %>%
    mutate(Detected = as.integer(stringr::str_detect(Validation_TaxonIDs, Validated_Grp)), 
           SnippetID = paste(FileId, Site, Date, Orig_Start = Orig_Start_Time, Orig_End = Orig_End_Time, sep = "_")) %>%
    select(Validated_Grp, Validation_Species, Validated_Grp_score, SnippetID, Detected) 
    
}



files_to_read_2 <- list.files("data/validated_calls/2021-2024")
sp_read_order_2 <- stringr::str_remove(files_to_read_2, "_validation.csv")

validated_calls_2 <- list()
for(i in 1:nspecies) {
  validated_calls_2[[sp_read_order_2[i]]] <- validated_calls[[sp_read_order_2[i]]] %>% bind_rows(readr::read_csv(paste0("data/validated_calls/2021-2024/", files_to_read_2[i]), 
                                                         show_col_types = F) %>%
    mutate(Validated_Grp = as.character(species_list[[sp_read_order_2[i]]]),
           Validation_Species = sp_read_order_2[i],
           Validated_Grp_score = !!sym(sp_read_order_2[i]), 
           Date = as.Date(Date, "%d/%m/%Y")) %>%
    mutate(Detected = as.integer(stringr::str_detect(Validation_TaxonIDs, Validated_Grp)), 
           SnippetID = paste(FileId, Site, Date, Orig_Start = Orig_Start_Time, Orig_End = Orig_End_Time, sep = "_")) %>%
    select(Validated_Grp, Validation_Species, Validated_Grp_score, SnippetID, Detected))
    
}

records_validated <- list()

for(x in names(species_list)) {
  records_validated[[x]] <- left_join(site_records_combined[[x]], validated_calls_2[[x]]) %>%
    mutate(Detected = coalesce(Detected, 2L)) %>%
    arrange(Site_Year)
}

validated_calls_combined <- bind_rows(records_validated) %>%
  filter(Detected != 2) %>%
  rowwise() %>%
  mutate(Prob_f = case_when(Validated_Grp_score == 1 ~ 0.999,
                            Validated_Grp_score == 0 ~ 0.001,
                     TRUE ~ Validated_Grp_score),
        Prob_One_M = qlogis(Prob_f)) 

validated_calls_combined %>%
  mutate(Detected = as.factor(Detected)) %>%
  filter(Validation_Species != "Common Spadefoot Toad") %>%
  ggplot() +
  geom_density(aes(x = Prob_f, fill = Detected, colour = Detected), position = "jitter", alpha = 0.25) +
  facet_wrap(~Validation_Species) +
  xlab("Classifier-assigned probability") +
  delwp_theme() 

```

# Format data  

For the model, we filter out common spadefoot toad as we have no validated detections for this species. Below we format the acoustic, site, and nightly data into a format that can be used in the custom STAN model. 

```{r formatdata, message = F, warning = F}
# Generate STAN data 
sp_out <- c("Common Spadefoot Toad") 
            # "Pobblebonk Frog", 
            # "Spotted Marsh Frog (race unknown)")
species_list_cleaned <- species_list[which(!(names(species_list) %in% sp_out))]
records_validated <- records_validated[which(!(names(records_validated) %in% sp_out))]

nspecies_cleaned <- length(species_list_cleaned)

sp_l_col_cleaned <- paste0("sp_", species_list_cleaned)

# joined data
joined_data <- list()
site_data <- list()

# indexes for species
site_names <- unique(records_validated[[1]]$Site_Year)
nsites <- length(unique(records_validated[[1]]$Site_Year))

empty_array <- array(dim = c(nsites, length(species_list_cleaned)))
start_idx_0 <- empty_array
end_idx_0 <- empty_array
start_idx_1 <- empty_array
end_idx_1 <- empty_array
start_idx_2 <- empty_array
end_idx_2 <- empty_array
any_seen <- empty_array
site_idx_start <- empty_array
site_idx_end <- empty_array


call_rate_matrix <- list()
occ_matrix <- list()

for(i in 1:length(species_list_cleaned)) {

joined_data[[i]] <- records_validated[[i]] %>%
  inner_join(daily_vars %>% na.omit()) %>%
  mutate(Prob_One = Validated_Grp_score,
         Site_f = as.integer(factor(Site_Year)), 
         Prob_f = case_when(Prob_One == 1 ~ 0.999, 
                            Prob_One == 0 ~ 0.001, 
                            TRUE ~ Prob_One),
         Prob_One_M = qlogis(Prob_f), 
         weight = case_when(Detected == 2 ~ 0.01,
                            TRUE ~ 1)) %>%
  group_by(Site_f) %>%
  mutate(any_seen = case_when(any(Detected == 1) ~ 1, 
                              TRUE ~ 0)) %>%
  slice_sample(n = 10, weight_by = weight) %>%
  ungroup() %>%
  arrange(Site_f, Detected) 

site_data[[i]] <- joined_data[[i]] %>%
  mutate(Loc = as.factor(Site) %>% as.integer(), 
         Cluster = as.factor(str_remove_all(Site, "_AM1|_AM2")) %>% as.integer()) %>%
  select(Site_f, Loc, Cluster, Year, Site_Year, any_seen, Site) %>%
  distinct() %>%
  arrange(Site_f) %>% 
  left_join(site_vars, by = "Site_Year")

# occupancy model
psi_form <- ~ scale(sqrt(wet)) + scale(pv) + scale(sqrt(water)) # + FloodScore
occ_matrix[[i]] <- model.matrix(psi_form, data = site_data[[i]])

# Daily precipitation matrix
theta_form <- ~ scale(sqrt(Precipitation)) + scale(tmin) + scale(sqrt(wet)) + scale(sqrt(water)) + scale(pv) #+ cosDay + sinDay
call_rate_matrix[[i]] <- model.matrix(theta_form, data = joined_data[[i]])

for(j in 1:nsites) {
  start_idx_0[j,i] <- min(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 0))
  end_idx_0[j,i] <- max(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 0))
  
  start_idx_1[j,i] <- min(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 1))
  end_idx_1[j,i] <- max(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 1))
  
  start_idx_2[j,i] <- min(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 2))
  end_idx_2[j,i] <- max(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 2))
    
  site_idx_start[j,i] <- min(which(joined_data[[i]]$Site_f == j))
  site_idx_end[j,i] <- max(which(joined_data[[i]]$Site_f == j))
}

start_idx_0[is.infinite(start_idx_0[,i]),i] <- 0
start_idx_1[is.infinite(start_idx_1[,i]),i] <- 0
start_idx_2[is.infinite(start_idx_2[,i]),i] <- 0

end_idx_0[is.infinite(end_idx_0[,i]),i] <- 0
end_idx_1[is.infinite(end_idx_1[,i]),i] <- 0
end_idx_2[is.infinite(end_idx_2[,i]),i] <- 0

any_seen[,i] <- as.integer(start_idx_1[,i] != 0)
}
```

In addition to the validation data, we also have previous validations at a site-level that notes whether any calls were detected at that site in that year. These are included in the model as if restricts the probability space we need to marginalize over for sites that have at least one detection. 

```{r anyseen}
# read in updated any seen validations 
any_seen_add <- readr::read_csv("data/any_seen/sp_detect_per_site_per_year.csv") %>% 
      mutate(`Site_AM` = case_when(Site_AM == "Little Rushy Swamp_AM1" ~ "Little Rushy_AM1",
                              Site_AM == "Little Rushy Swamp_AM2" ~ "Little Rushy_AM2",
                              Site_AM ==  "Hughes Crossing_AM1" ~ "Hughs Crossing_AM1",
                              Site_AM ==  "Hughes Crossing_AM2" ~ "Hughs Crossing_AM2",
                              Site_AM == "Reed Beds Swamp_AM1" ~ "Reed Bed Swamp_AM1",
                              Site_AM == "Reed Beds Swamp_AM2" ~ "Reed Bed Swamp_AM2",
                              Site_AM == "Wathours Lagoon_AM1" ~ "Wathours_AM1",
                              Site_AM == "Wathours Lagoon_AM2" ~ "Wathours_AM2",
                              TRUE ~ `Site_AM`)) %>%
  mutate(Site_Year = paste(Site_AM, Year, sep = "_")) %>%
  arrange(Site_Year) 

setdiff(site_names, any_seen_add$Site_Year) %>% paste(collapse = "\n") %>% cat()

any_seen_clean <- any_seen_add %>% filter(Site_Year %in% site_names) %>%
  select(`Barking Marsh Frog`,
         `Eastern Sign-bearing Froglet` = `Eastern Sign-Bearing Froglet`,
         `Common Froglet`, 
         `Peron's Tree Frog`, 
         `Pobblebonk Frog` = Pobblebonk,
         `Spotted Marsh Frog (race unknown)` = `Spotted Marsh Frog`) %>%
  as.matrix()

any_seen_joined <- any_seen_clean

for(j in 1:ncol(any_seen_clean)) {
  for(i in 1:nrow(any_seen_clean))
    any_seen_joined[i,j] <- any_seen[i,j] #max(any_seen[i,j], any_seen_clean[i,j])
}
```

We then combine all the data into a list, which will be used by the STAN model 

```{r stanlist}
nfiles <- nrow(joined_data[[1]])
scores <- lapply(joined_data, function(x) pull(x, Prob_One_M)) 
site_list <- lapply(joined_data, function(x) pull(x, Site_f)) 
loc_code <- lapply(site_data, function(x) pull(x, Loc)) 
cluster_code <- lapply(site_data, function(x) pull(x, Cluster)) 
year_code <- lapply(site_data, function(x) pull(x, Year)) 
detected <- lapply(joined_data, function(x) pull(x, Detected))

nyear <- length(unique(unlist(year_code)))
nloc <- length(unique(unlist(loc_code)))
nclusters <- length(unique(unlist(cluster_code)))


stan_data <- list(nfiles = nfiles, 
                  nsites = nsites, 
                  nspec = nspecies_cleaned,
                  nyear = nyear, 
                  nloc = nloc, 
                  nclusters = nclusters,
                  score = array(unlist(scores),dim=c(nfiles,nspecies_cleaned)), 
                  loc_code = array(unlist(loc_code),dim=c(nsites,nspecies_cleaned)), 
                  cluster_code = array(unlist(cluster_code),dim=c(nsites,nspecies_cleaned)), 
                  year_code = array(unlist(year_code),dim=c(nsites,nspecies_cleaned)), 
                  year_counts = unname(apply(array(unlist(year_code),dim=c(nsites,nspecies_cleaned)),2,table)),
                  year_counts_file = unname(as.integer(table(joined_data[[1]]$Year))),
                  site_idx_start = site_idx_start, 
                  site_idx_end = site_idx_end,
                  start_idx_0 = start_idx_0, 
                  end_idx_0 = end_idx_0,
                  start_idx_1 = start_idx_1, 
                  end_idx_1 = end_idx_1, 
                  start_idx_2 = start_idx_2, 
                  end_idx_2 = end_idx_2, 
                  any_seen = any_seen_joined, 
                  theta_mm = call_rate_matrix, 
                  theta_nc = ncol(call_rate_matrix[[1]]), 
                  psi_mm = occ_matrix, 
                  psi_nc = ncol(occ_matrix[[1]]), 
                  detected = array(unlist(detected),dim=c(nfiles,nspecies_cleaned)))

# stan_data$year_code[stan_data$year_code == 5] <- 4
stan_data_beta <- stan_data 
stan_data_beta$score <- array(unlist(lapply(joined_data, function(x) pull(x, Prob_f))),dim=c(nfiles,nspecies_cleaned))

saveRDS(stan_data, "data/stan_data.rds")

```

# STAN Model  

Below is the STAN model used in our analysis. It is a mult-species, multi-season model. 

```{cmdstan, file = "stan/continuous_model_ms_my_norm.stan", output.var = "cont_model_ms_norm", eval = FALSE}

```

```{r readmods, echo = F}
cont_model_ms_norm <- cmdstan_model("stan/continuous_model_ms_my_norm.stan")
cont_model_ms_beta <- cmdstan_model("stan/continuous_model_ms_my_beta.stan")
```

```{r modelparams}
ni <- 150 #samples
nw <- 150 #warmups
nc <- 8 #chains
```

```{r fitmods, eval = F}
# fit_norm <- cont_model_ms_norm$sample(data = stan_data,
#                          chains = nc,
#                          parallel_chains = nc,
#                          show_messages = TRUE,
#                          save_warmup = FALSE,
#                          iter_sampling = ni,
#                          iter_warmup = nw, refresh = 20)
# 
# fit_norm$save_object("outputs/fit_norm.rds")

fit_beta <- cont_model_ms_beta$sample(data = stan_data_beta,
                         chains = nc,
                         parallel_chains = nc,
                         show_messages = TRUE,
                         save_warmup = FALSE,
                         iter_sampling = ni,
                         iter_warmup = nw, refresh = 20)

fit_beta$save_object("outputs/fit_beta.rds")
```

```{r readinmods}
fit_beta <- readRDS("outputs/fit_beta.rds")
```

# Model analysis 

## Posterior predictive checks  

Posterior predictive checks can be used to compare model predictions to the original data supplied to the model. Here we compare summary statistics for the probability scores supplied to the model. Ideally, predictions from the model are able to align with the supplied data. We compare 25%, 50% and 75% confidence intervals for the 'probability scores'.

```{r PPC, fig.height=24, fig.width=18, message = F}
q95 <- function(x) quantile(x, 0.95, na.rm = T)
q25 <- function(x) quantile(x, 0.25, na.rm = T)
q50 <- function(x) quantile(x, 0.5, na.rm = T)
q75 <- function(x) quantile(x, 0.75, na.rm = T)
sd_90 <- function (x, na.rm = FALSE) {
  quants <- quantile(x, c(0.05, 0.95), na.rm = T)
  x <- x[x < quants[2] & x > quants[1]]
  sqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x),
    na.rm = na.rm))
}

species_list_cleaned2 <- species_list_cleaned
names(species_list_cleaned2)[6] <- "Spotted Marsh Frog"

ppc_plots_25 <- list()
ppc_plots_50 <- list()
ppc_plots_75 <- list()
for(i in 1:length(species_list_cleaned)) {
ppc_plots_25[[i]] <- posterior_checks_multispecies(model = fit_beta, 
                              model_data = joined_data, 
                              species_index = i,
                              stat = "q25", 
                              title = paste0("25% quantile ", names(species_list_cleaned2)[i]))

ppc_plots_50[[i]] <- posterior_checks_multispecies(model = fit_beta, 
                              model_data = joined_data, 
                              species_index = i,
                              stat = "q50", 
                              title = paste0("50% quantile ", names(species_list_cleaned2)[i]))

ppc_plots_75[[i]] <- posterior_checks_multispecies(model = fit_beta, 
                              model_data = joined_data, 
                              species_index = i,
                              stat = "q75", 
                              title = paste0("75% quantile ", names(species_list_cleaned2)[i]))

}

ppc_grid <- cowplot::plot_grid(plotlist = c(ppc_plots_25, ppc_plots_50, ppc_plots_75), nrow = 6, byrow = F)

ggsave(plot = ppc_grid, filename = "outputs/plots/ppcs_beta.pdf", width = 6000, height = 8000, units = "px")

ppc_grid
```


```{r, fig.width = 18, fig.height = 30, eval = FALSE}
psi_summary <- fit_beta$summary("psi")

for(i in 1:length(species_list_cleaned)) {
psi_summary_species <- psi_summary %>%
  filter(str_detect(variable, pattern = paste0(",", i,"]"))) %>% bind_cols(site_data[[i]]) %>%
  mutate(label = paste0("Det = ", any_seen, ", Occ = ", round(mean,2), " [", round(q5,2), " - ", round(q95,2), "]"))

heatmap <- joined_data[[i]] %>% 
  mutate(yday = yday(Date)) %>%
  ggplot() + 
  geom_tile(aes(x = yday, y = Site_Year, fill = Prob_f, colour = Prob_f)) + 
  geom_text(aes(x = 100, y = Site_Year, label = label), data = psi_summary_species, size = 4) +
  scale_fill_gradient(low = "#fff5f0", high = "#67000d") +
  scale_colour_gradient(low = "#fff5f0", high = "#67000d") + 
  ggtitle(label = names(species_list_cleaned)[i])

ggsave(plot = heatmap, filename = paste0("outputs/plots/heatmap/score_heatmap_",names(species_list_cleaned)[i],".pdf"), width = 5000, height = 10000, units = "px")
}
```

# Model summary 

```{r, eval = FALSE}
psi_summary <- fit_norm$summary("psi")
psi_summary_sp <- list()
for(i in 1:nspecies_cleaned) {
  var_names <- paste0("psi[",1:nsites, ",", i, "]")
  psi_summary_sp[[i]] <- psi_summary %>% 
    filter(variable %in% var_names) %>%
    mutate(species = names(species_list_cleaned)[i], 
           any_seen = stan_data$any_seen[,i])
}
psi_summary <- bind_rows(psi_summary_sp)

psi_summary %>%
  group_by(any_seen, species) %>%
  summarise(psi = mean(mean))
```

```{r, eval = FALSE}
# phenology summary 
theta_phen <- fit_norm$summary("theta_phen") 

theta_phen_spec <- list()

for(i in 1:nspecies_cleaned) {
  theta_phen_spec[[i]] <- theta_phen %>%
  filter(str_detect(variable, paste0(",",i,"]"))) %>%
    mutate(Species = names(species_list_cleaned)[i], 
           Day = 1:365, 
           Date = seq.Date(from = as.Date("2022-09-01"), to = as.Date("2023-08-30"), length.out = 365))
}

theta_phen_joined <- bind_rows(theta_phen_spec)

theta_phen_joined %>% ggplot(aes(x = Day, 
                                 y = mean, 
                                 colour = Species)) +
  geom_line() +
  scale_x_continuous(limits = c(1, 120))
```

```{r, fig.width = 10, fig.height = 6}
nbeta_theta <- ncol(call_rate_matrix[[1]])
beta_theta_names <- labels(call_rate_matrix[[1]])[[2]]
theta_summary <- fit_norm$summary("beta_theta")
theta_summary_sp <- list()
for(i in 1:nspecies_cleaned) {
  var_names <- paste0("beta_theta[",i, ",", 1:nbeta_theta, "]")
  theta_summary_sp[[i]] <- theta_summary %>% 
    filter(variable %in% var_names) %>%
    mutate(species = names(species_list_cleaned)[i], 
           param = beta_theta_names)
}
theta_summary_combined <- bind_rows(theta_summary_sp)


theta_summary_combined %>% 
  ggplot(aes(x = param, y = mean, colour = species)) +
  geom_pointrange(aes(ymin = q5, ymax = q95), position = position_dodge2(width = 0.5, padding = 0.5))
```

## Call recogniser performance  

Using our model we can visualise how our statistical model is able to delineate validated calls into detections an non-detections. Our model determines the performance of the acoustic classifier by the estimation of two hierarchical parameters for each species. These parameters estimate the 'classifier probability score' ($\mu$) when there is no call present in the snippet ($\mu_{det\ =\ 0}$) and when there is a call present in the snippet ($\mu_{det\ =\ 1}$). At the very least, the classifier should have a higher probability score for detections than non-detections ($\mu_{det\ =\ 1}$ > $\mu_{det\ =\ 0}$).

```{r, fig.height = 8, message = FALSE}
mu_summary <- fit_beta$draws("mu_pred", format = "df") 
mu_summary_long <- pivot_longer(mu_summary, cols = 1:12)
mu_summary_long$`Validated detection` <- rep(c("Non-detection", "Detection"), length.out = nrow(mu_summary_long))
mu_summary_long$Species <- rep(names(species_list_cleaned), each = 2, length.out = nrow(mu_summary_long))

ggplot(data = mu_summary_long) +
  stat_density_ridges(aes(x = value, 
                          y = `Validated detection`, 
                          fill = 0.5 - abs(0.5 - stat(ecdf))), 
                      quantile_lines = TRUE, quantiles = 2, geom = "density_ridges_gradient", calc_ecdf = TRUE) +
  scale_fill_gradient(name = "Posterior density", low = delwp_cols[3], high = delwp_cols[1]) +
  scale_x_continuous(expand = c(0, 0), limits = c(0,1), name = "Classifier probability score") +
  scale_y_discrete(expand = c(0, 0)) +
  coord_cartesian(clip = "off") +
  facet_wrap(~Species) + 
  delwp_theme() +
  theme(panel.spacing = unit(2, "lines"))
```
Based on these distributions we see that the classifier is more often than not able to delineate true calls from unlikely calls. However. we also see that these distributions overlap. This means there is a relatively large probaility space where a 'classifier probability score' could reasonable both be a non-detection or a detection. 

## Covariates impacting occupancy  

We investigated whether several hydrological covariates impacted site occupancy for each species. These variables can vary across time (over survey years) and across space (over different sites). 

```{r FixedEffects, fig.height=15, fig.width = 8, warning = FALSE, message = FALSE, fig.cap="Marginal response curves for the various fixed-effect parameters used in the model."}
# beta_summaries 
beta_summaries <- fit_beta$summary("beta_psi")

beta_table_summary <- beta_summaries %>% 
  mutate(species = factor(rep(names(species_list_cleaned), times = 4), levels = names(species_list_cleaned)), 
         covariate = rep(c("(Intercept)",
              "Wetness", 
              "Photosynthetic Vegetation", 
              "Water"), each = length(species_list_cleaned))) %>%
  arrange(species) %>%
  select(species, covariate, mean, sd, q5, q95)

format_cov <- function(sp, b, data = beta_summaries, variable = "beta_psi") {
  fd <- data %>%
    filter(variable == !!paste0(variable, "[", sp, ",",b+1,"]"))
  
  paste0("$\\beta$ = ",
         round(fd$mean, 2), 
        " [90% CI: ", 
        round(fd$q5, 2), 
        " – ",
        round(fd$q95, 2),
        "]")
}

ab_joined_list <- list()
for(j in 1:length(species_list_cleaned)) {
beta_draws <- fit_beta$draws("beta_psi", format = "df")

which_sp <- which(stringr::str_detect(string = colnames(beta_draws),
                                    pattern = paste0("beta_psi\\[", j)))

beta_draws <- beta_draws[,which_sp]

ab_marginal_effects <- list()
ab_plot <- list()

ab_to_use <- psi_form

phi_vars <- unlist(stringr::str_remove_all(string = labels(terms(psi_form)), pattern =  "log|scale|sqrt|\\)|\\("))
phi_logs <- unlist(stringr::str_detect(string = labels(terms(psi_form)), pattern =  "log\\("))
phi_sqrts <- c(0.5,1,0.5)

phi_labs <- c("Wetness", 
              "Photosynthetic Vegetation", 
              "Water")

fac <- c(FALSE, FALSE, FALSE)

for(i in 1:length(phi_vars)) {
ab_marginal_effects[[i]] <- marginal_effects_cmd(beta_draws, 
                                              param = "beta_psi", species_index = j,
                                              param_number = i+1, log = phi_logs[i],
                                     model_data = site_data[[j]], 
                                     abundance = FALSE,
                                     pwr = phi_sqrts[i],
                                     model_column = phi_vars[i], 
                                     transition = FALSE) %>%
  mutate(species = names(species_list_cleaned)[j])

}
ab_joined_list[[j]] <- bind_rows(ab_marginal_effects)
}

all_me_data <- bind_rows(ab_joined_list) %>%
  mutate(param = case_when(param == "wet" ~ "Wetness", 
                           param == "pv" ~ "Photosynthetic Vegetation", 
                           param == "water" ~ "Water"), 
         species = case_when(species == "Spotted Marsh Frog (race unknown)" ~ "Spotted Marsh Frog", TRUE ~ species))
  

cov_plot <- marginal_effects_plot_cmd_all(all_me_data %>% 
                                            mutate(param = factor(param, levels = phi_labs)), 
                          col = "DarkGreen", 
                          factor = FALSE,
                          ylab = "Contribution to occupancy") +
   ggplot2::facet_grid(species~param, scales = "free") +
  delwp_theme() +
  theme(strip.text = ggtext::element_markdown()) +
  xlab("Covariate value")

ggsave(plot = cov_plot, filename = "outputs/plots/occupancy_covariate_effects.png", width = 2000, height = 3000, units = "px")

cov_plot

```

```{r FixedEffects, fig.height=15, fig.width = 12, warning = FALSE, message = FALSE, fig.cap="Marginal response curves for the various fixed-effect parameters used in the model."}
# beta_summaries 
theta_summaries <- fit_beta$summary("beta_theta")

theta_table_summary <- theta_summaries %>% 
  mutate(species = factor(rep(names(species_list_cleaned), times = 6), levels = names(species_list_cleaned)), 
         covariate = rep(c("(Intercept)",
                           "Precipitation", 
                           "Minimum Temperature",
                           "Wetness", 
                           "Water",
                           "Photosynthetic Vegetation"), each = length(species_list_cleaned))) %>%
  arrange(species) %>%
  select(species, covariate, mean, sd, q5, q95)

ab_joined_list <- list()
for(j in 1:length(species_list_cleaned)) {
theta_draws <- fit_beta$draws("beta_theta", format = "df")

which_sp <- which(stringr::str_detect(string = colnames(theta_draws),
                                    pattern = paste0("beta_theta\\[", j)))

beta_draws <- theta_draws[,which_sp]

ab_marginal_effects <- list()
ab_plot <- list()

ab_to_use <- theta_form

phi_vars <- unlist(stringr::str_remove_all(string = labels(terms(ab_to_use)), pattern =  "log|scale|sqrt|\\)|\\("))
phi_logs <- unlist(stringr::str_detect(string = labels(terms(ab_to_use)), pattern =  "log\\("))
phi_sqrts <- c(1,1,1,1,1)

phi_labs <- c("Precipitation", 
              "Minimum Temperature",
              "Wetness", 
              "Water",
              "Photosynthetic Vegetation")

fac <- c(FALSE, FALSE, FALSE, FALSE, FALSE)

for(i in 1:length(phi_vars)) {
ab_marginal_effects[[i]] <- marginal_effects_cmd(theta_draws, 
                                              param = "beta_theta", species_index = j,
                                              param_number = i+1, log = phi_logs[i],
                                     model_data = joined_data[[j]], 
                                     abundance = FALSE,
                                     pwr = phi_sqrts[i],
                                     model_column = phi_vars[i], 
                                     transition = FALSE) %>%
  mutate(species = names(species_list_cleaned)[j])

}
ab_joined_list[[j]] <- bind_rows(ab_marginal_effects)
}

all_theta_data <- bind_rows(ab_joined_list) %>%
  mutate(param = case_when(param == "wet" ~ "Wetness", 
                           param == "pv" ~ "Photosynthetic Vegetation", 
                           param == "water" ~ "Water", 
                           param == "tmin" ~ "Minimum Temperature", 
                           TRUE ~ param), 
         species = case_when(species == "Spotted Marsh Frog (race unknown)" ~ "Spotted Marsh Frog", TRUE ~ species))
  

theta_plot <- marginal_effects_plot_cmd_all(all_theta_data %>% 
                                            mutate(param = factor(param, levels = phi_labs)), 
                          col = "DarkGreen", 
                          factor = FALSE,
                          ylab = "Contribution to call rate") +
   ggplot2::facet_grid(species~param, scales = "free") +
  delwp_theme() +
  theme(strip.text = ggtext::element_markdown()) +
  xlab("Covariate value")

ggsave(plot = theta_plot, filename = "outputs/plots/call_rate_covariate_effects.png", width = 2800, height = 3000, units = "px")

theta_plot

```

```{r}
score_pred <- fit$draws(paste0("score_pred[", 1:stan_data$nfiles, ",1]"), format = "matrix")


plotdens <- ppc_hist(y = lapply(joined_data, function(x) pull(x, Prob_f))[[1]], yrep = score_pred)
```

```{r avrates}
zhat <- fit_norm$summary("z_hat")
zhat$year <- rep(sort(unique(site_metadata_sf$`Monitoring year`)), times = length(species_list_cleaned))
zhat$species <- rep(names(species_list_cleaned), each = length(unique(site_metadata_sf$`Monitoring year`)))

theta_hat <- fit_norm$summary("theta_hat")
theta_hat$year <- zhat$year
theta_hat$species <- zhat$species

zhat_draws <- fit_beta$draws("z_hat", format = "matrix") %>%
  as.data.frame() %>%
  pivot_longer(cols = everything()) %>%
  mutate(year = rep(sort(unique(site_metadata_sf$`Monitoring year`)), length.out = nrow(.)),
         species = rep(names(species_list_cleaned), 
                       each = length(unique(site_metadata_sf$`Monitoring year`)),
                       length.out = nrow(.)))



dfas <- list()
for(i in 1:nspecies_cleaned) {
dfas[[i]] <- data.frame(any_seen = stan_data$any_seen[,i], 
                   yearf = stan_data$year_code[,i]) %>%
  group_by(yearf) %>%
  summarise(det = sum(any_seen), 
            tot = n(), 
            rate = det/tot) %>%
  ungroup() %>%
  mutate(year = sort(unique(site_metadata_sf$`Monitoring year`)),
         species = names(species_list_cleaned)[i])
}

dfas_comb <- bind_rows(dfas)


theta_hat_draws <- fit_beta$draws("theta_hat", format = "matrix") %>%
  as.data.frame() %>%
  pivot_longer(cols = everything()) %>%
  mutate(year = rep(sort(unique(site_metadata_sf$`Monitoring year`)), length.out = nrow(.)),
         species = rep(names(species_list_cleaned),
                       each = length(unique(site_metadata_sf$`Monitoring year`)),
                       length.out = nrow(.)))

theta_hat_draws %>%
  ggplot() +
  geom_violin(aes(x = value, y = year)) +
  facet_wrap(~species)

zhat_draws %>%
  ggplot() +
  geom_violin(aes(x = value, y = year)) +
  geom_point(aes(x = rate, y = year), data = dfas_comb, shape = 21, size = 2, fill = "DarkRed") +
  facet_wrap(~species)
```

```{r, fig.width = 5, fig.height = 10}
rel_ab_draws <- fit_beta$draws("rel_ab_mean", format = "matrix") %>%
  as.data.frame() %>%
  pivot_longer(cols = everything()) %>%
  mutate(year = rep(sort(unique(site_metadata_sf$`Monitoring year`)), length.out = nrow(.)),
         species = rep(names(species_list_cleaned), 
                       each = length(unique(site_metadata_sf$`Monitoring year`)),
                       length.out = nrow(.)))

rel_ab_draws %>%
  ggplot() +
  geom_violin(aes(x = year, y = value)) +
  facet_wrap(~species, ncol = 1)
```

