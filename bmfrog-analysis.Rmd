---
title: "BMFrog Analysis"
output: html_document
date: "2024-03-07"
---

```{r setup}
library(tidyverse)
library(sf)
library(mapview)
library(terra)
library(readr)
library(readxl)
library(cmdstanr)
library(bayesplot)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
options(mc.cores=4)
register_knitr_engine(override = FALSE)
```

# Load Site Data  

```{r}
site_metadata <- read_excel("data/TLM_Barmah-Millewa_2022-23_AM_deployment_metadata.xlsx")
site_metadata_sf <- site_metadata %>% 
  st_as_sf(coords = c("Easting", "Northing"), crs = 28355) %>%
  mutate(`Site name` = case_when(`Site name` == "Warri" ~ "Warri Yards", 
                                 TRUE ~ `Site name`), 
         Site = paste(`Site name`, AM_no, sep = "_AM"), 
         No_survey_nights = as.numeric(No_survey_nights)) %>%
  arrange(`Site`) %>%
  filter(!is.na(No_survey_nights) & No_survey_nights > 0)

#nights deployed 
nights_dep <- list()
for(i in 1:nrow(site_metadata_sf)) {
  nights_dep[[i]] <- data.frame(Site = site_metadata_sf$Site[i], 
                           Date = seq.Date(from = as.Date(site_metadata_sf$Date_deployed[i]), 
                                           by = "1 day", 
                                           length.out = site_metadata_sf$No_survey_nights[i]))
}
nights_dep_joined <- bind_rows(nights_dep)
```

```{r}
# load in daily precip 
precipitation_daily <- c(terra::rast("data/precip_2022.nc"), terra::rast("data/precip_2023.nc"))
precipitation_daily_df <- bind_cols(site_metadata_sf$Site, 
                                    extract(precipitation_daily, vect(site_metadata_sf)))
colnames(precipitation_daily_df) <- c("Site","ID", as.character(seq.Date(from = as.Date("2022-01-01"), 
                                                     to = as.Date("2023-12-31"), by = "day")))

precipitation_daily_long <- pivot_longer(precipitation_daily_df, cols = 3:732, names_to = "Date", values_to = "Precipitation") %>%
  select(-ID) %>%
  mutate(Date = as.Date(Date)) %>%
  right_join(nights_dep_joined)
```


# All site records 
```{r}
site_records <- read_csv("data/TLM_2022-23_Barmah-Millewa_FrogsEnsemble1_2023.csv") %>%
  rename(Site_prefix = Site) %>%
  tidyr::separate_wider_delim(cols = "Filename", delim = "/", 
                              names = c(NA,NA,NA,"Site", NA))
```

# Validated site records 

```{r}
site_validation <- read_excel("data/TLM_Barmah-Millewa_2022-23_validations_ALL.xlsx") %>%
 mutate(Site = case_when(Site == "Warri" ~ "Warri Yards", 
                         TRUE ~ Site),
        Site = paste(Site, AM, sep = "_AM"))
```

```{r}
# Generate STAN data 
species_list <- c(#`Barking Marsh Frog` = 13059, 
                  `Eastern Sign-bearing Froglet` = 13131,
                  #`Common Froglet` = 13134,
                  # `Sloane's Froglet` = 13135,
                  `Peron's Tree Frog` = 13204)#, 
                  #`Pobblebonk Frog` = 63913)
                  # `Growling Grass Frog` = 13207)

nspecies <- length(species_list)

sp_l_col <- paste0("sp_", species_list)

species_cols <- str_extract_all(last(colnames(site_records)), "[-+]?[0-9]+")[[1]] 

#list of species codes and names
class_list<-read_csv("data/ClassList.csv")

#data with column containing string of category probabilities from the AI
names(site_records)[16]<-"probvec"

#clean and split the probabilities
probs <- site_records %>% select(probvec) %>%
  mutate(probvec=str_remove(probvec, "\n")) %>%
  mutate(probvec=str_remove(probvec, "\\[")) %>%
  mutate(probvec=str_remove(probvec, "\\]")) %>%
  mutate(probvec=str_squish(probvec)) %>%
  separate(probvec, into=paste0("sp_",class_list$TaxonId), sep="\\s") %>%
  mutate(across(`sp_-2`:`sp_525739`, as.numeric))

# perons probs 
frog_probs <- probs %>%
  dplyr::select(all_of(sp_l_col))

site_records_date <- site_records %>% 
  mutate(Date = as.Date(Date_evening_of, "%d/%m/%Y"), 
         DateTime = as.POSIXct(paste(Date, Time))) %>%
  bind_cols(frog_probs)

# joined data
joined_data <- list()

# indexes for species
nsites <- length(site_metadata_sf$Site)
empty_array <- array(dim = c(nsites, length(species_list)))
start_idx_0 <- empty_array
end_idx_0 <- empty_array
start_idx_1 <- empty_array
end_idx_1 <- empty_array
start_idx_2 <- empty_array
end_idx_2 <- empty_array
any_seen <- empty_array

call_rate_matrix <- list()

for(i in 1:length(species_list)) {

all_records_filt <- site_records_date %>%
  group_by(Site, Date) %>%
  summarise(Prob_One = max(!!sym(sp_l_col[i])), 
            FileId = FileId[which(!!sym(sp_l_col[i]) == max(!!sym(sp_l_col[i])))], 
            Orig_Start_Time = Orig_Start_Time[which(!!sym(sp_l_col[i]) == max(!!sym(sp_l_col[i])))], 
            Orig_End_Time = Orig_End_Time[which(!!sym(sp_l_col[i]) == max(!!sym(sp_l_col[i])))]) %>%
  slice(1) %>%
  ungroup()

validation_records_clean <- site_validation %>% 
  select(FileId, Orig_Start_Time, Orig_End_Time, Site, Detected = !!sym(names(species_list)[i])) %>%
  left_join(site_records_date %>%
              select(FileId, Orig_Start_Time, Orig_End_Time, Site, Date, Prob_One = !!sym(sp_l_col[i])))

joined_data[[i]] <- full_join(all_records_filt, validation_records_clean) %>%
  group_by(Site, Date) %>%
  arrange(desc(Detected), Prob_One) %>%
  slice(1) %>%
  right_join(precipitation_daily_long) %>%
  mutate(Detected = replace_na(Detected, -2), 
         Prob_One = replace_na(Prob_One, 0)) %>%
  mutate(Detected = abs(Detected), 
         Site_f = as.integer(factor(Site)), 
         Prob_One_M = qlogis(case_when(Prob_One == 1 ~ 0.999, 
                                Prob_One == 0 ~ 0.001, 
                                TRUE ~ Prob_One))) %>%
  arrange(Site_f, Detected) 

# Daily precipitation matrix
theta_form <- ~ scale(Precipitation)
call_rate_matrix[[i]] <- model.matrix(theta_form, data = joined_data[[i]])

for(j in 1:nsites) {
  start_idx_0[j,i] <- min(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 0))
  end_idx_0[j,i] <- max(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 0))
  
  start_idx_1[j,i] <- min(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 1))
  end_idx_1[j,i] <- max(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 1))
  
  start_idx_2[j,i] <- min(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 2))
  end_idx_2[j,i] <- max(which(joined_data[[i]]$Site_f == j & joined_data[[i]]$Detected == 2))
}

start_idx_0[is.infinite(start_idx_0[,i]),i] <- 0
start_idx_1[is.infinite(start_idx_1[,i]),i] <- 0
start_idx_2[is.infinite(start_idx_2[,i]),i] <- 0

end_idx_0[is.infinite(end_idx_0[,i]),i] <- 0
end_idx_1[is.infinite(end_idx_1[,i]),i] <- 0
end_idx_2[is.infinite(end_idx_2[,i]),i] <- 0

any_seen[,i] <- as.integer(start_idx_1[,i] != 0)
}
```


```{r}
nfiles <- nrow(joined_data[[1]])
scores <- lapply(joined_data, function(x) pull(x, Prob_One_M)) 

stan_data <- list(nfiles = nfiles, 
                  nsites = nsites, 
                  nspec = nspecies,
                  score = array(unlist(scores),dim=c(nfiles,nspecies)), 
                  start_idx_0 = start_idx_0, 
                  end_idx_0 = end_idx_0,
                  start_idx_1 = start_idx_1, 
                  end_idx_1 = end_idx_1, 
                  start_idx_2 = start_idx_2, 
                  end_idx_2 = end_idx_2, 
                  any_seen = any_seen, 
                  theta_mm = call_rate_matrix, 
                  theta_nc = ncol(call_rate_matrix[[1]]))

```


```{r}
cont_model <- cmdstan_model("stan/continuous_model.stan")
cont_model_ms <- cmdstan_model("stan/continuous_model_ms.stan")
```

```{r modelparams}
ni <- 2000 #samples
nw <- 2000 #warmups
nc <- 8 #chains
```

```{r}
fit <- cont_model_ms$sample(data = stan_data,
                         chains = nc,
                         parallel_chains = nc,
                         show_messages = TRUE,
                         save_warmup = FALSE,
                         iter_sampling = ni,
                         iter_warmup = nw, adapt_delta = 0.99)
```

```{r}
psi_summary <- fit$summary("psi")
psi_summary$any_seen <- any_seen 

psi_summary %>%
  group_by(any_seen) %>%
  summarise(psi = mean(mean))

fit$summary("mu_alpha")
```

